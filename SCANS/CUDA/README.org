
* CUDA Files
  * sklansky1.cu
    1024 element scan in a single block using 32 threads. No Syncs!
    uses 32 * 3 elements of shared mem.
  * sklansky2.cu 
    1024 element scan in a single block using 16 threads. No Syncs!
    uses 32 * 2 elements of shared mem.
  * sklansky3.cu 
    4096 element scan in a single block using 32 threads. No Syncs! 
    uses 64 * 2 elements of shared mem.



* TODOs: 
  * Test for correctness. 
  * Implement large scan (millions of elements).
  * Benchmark for performance. 
  * Try to push for as many elements as possible (per blocks)  with no syncs. 
  * Then try to push for as many elements as possible globally with as few 
    global syncs as possible (kernel invocations).
  * investigate if some schema using atomic ops can limit global syncs even 
    further. 
  * Experiment with using __shfl commands to exchange data within a
    Warp without using any shared memory. (May influence implementation of
    the "local" part of the scans.
  * Experiment with other base-scans than Sklansky. 
  * load float2s instead of floats     
  * 64*64*4 = 16384. so entire work set fits in shared memory. 
    (if that is a good thing). This only gives room for 3 such blocks 
    in the shared mem of an MP. 

* Potential issues: 
  * Distribute phase: each thread performs 2*64 additions on global memory! 
      - Potentially better to do in shared memory ? 
  * Each "warp-scan" reads global memory and writes global memory. 
      - put entire work set in shared memory (enough room?).  
  
